{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NSAC - ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNMTZVkUvY+8alR9o0fCGMg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKZTkylySCcr"
      },
      "source": [
        "# Guide"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grkvUOWMJfVZ"
      },
      "source": [
        "## IMPORTS\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "## LOADING\n",
        "# Load vegetation data\n",
        "# Load rain data\n",
        "# Load altitude data\n",
        "# Load temperature data\n",
        "# X Load landslides data\n",
        "\n",
        "## DATA PREPROCESSING\n",
        "# create slope data from altitude\n",
        "# scale data in a 0 to 1 range\n",
        "\n",
        "# X clear landslides data from useless data\n",
        "# X filter values outside the specified region (50N - 40N, 115W - 125W)\n",
        "# X filter non-water related landslides\n",
        "# obtain data from other datasets to complete the positive values of the set\n",
        "\n",
        "## NEGATIVE DATA CREATION\n",
        "# Generate random location\n",
        "# Check if the location is available (h>0)\n",
        "# Generate random date/hour\n",
        "# Check if there wasn't a landslide\n",
        "\n",
        "## CREATE TRAINING, VALIDATION AND TEST SETS\n",
        "# stratified separation for validation and test sets\n",
        "# oversampling landslides events (x3?) to account for imbalance\n",
        "# shuffle training and validation\n",
        "\n",
        "## DEFINE MODEL\n",
        "# weighted loss function\n",
        "# dense deep neural network (architecture?)\n",
        "# binary classification (which function? sigmoid?)\n",
        "\n",
        "## TEST PREDICTIONS\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOszr2E9wJ_0"
      },
      "source": [
        "# Preparation of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5TTHGXbwa-c"
      },
      "source": [
        "## Landslides data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anKlBpUSaYSd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# list of landslide events from COOLR\n",
        "landslide_data = pd.read_csv('/content/nasa_global_landslide_catalog_point.csv')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebg0smiBatgk",
        "outputId": "9ce28537-63d4-4d7a-ac2e-1b6e37960e74"
      },
      "source": [
        "# Overlook at the dataset\n",
        "print(landslide_data.axes[1])\n",
        "# List of triggers\n",
        "landslide_data['landslide_trigger'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['OBJECTID', 'Shape', 'source_name', 'source_link', 'event_id',\n",
            "       'event_date', 'event_time', 'event_title', 'event_description',\n",
            "       'location_description', 'location_accuracy', 'landslide_category',\n",
            "       'landslide_trigger', 'landslide_size', 'landslide_setting',\n",
            "       'fatality_count', 'injury_count', 'storm_name', 'photo_link',\n",
            "       'comments', 'event_import_source', 'event_import_id', 'latitude',\n",
            "       'longitude', 'country_name', 'country_code', 'admin_division_name',\n",
            "       'gazetteer_closest_point', 'gazetteer_distance', 'submitted_date',\n",
            "       'last_edited_date'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "downpour                   4794\n",
              "unknown                    2842\n",
              "rain                       2716\n",
              "continuous_rain             780\n",
              "tropical_cyclone            563\n",
              "snowfall_snowmelt           147\n",
              "monsoon                     133\n",
              "mining                      103\n",
              "construction                 95\n",
              "earthquake                   95\n",
              "flooding                     81\n",
              "no_apparent_trigger          65\n",
              "freeze_thaw                  44\n",
              "other                        36\n",
              "dam_embankment_collapse      15\n",
              "leaking_pipe                 13\n",
              "volcano                       2\n",
              "vibration                     2\n",
              "Name: landslide_trigger, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoxOZIb9jFmA",
        "outputId": "eb931165-7a94-4e98-c951-678b78e43d3c"
      },
      "source": [
        "# Restricts the area to 50N125W - 40N115W\n",
        "us_landslides = landslide_data.loc[landslide_data['country_name'] == 'United States'].dropna(subset=['event_date'])\n",
        "region_landslides = us_landslides.loc[us_landslides['latitude'] > 40]\n",
        "region_landslides = region_landslides.loc[region_landslides['latitude'] < 50]\n",
        "region_landslides = region_landslides.loc[region_landslides['longitude'] > -125]\n",
        "region_landslides = region_landslides.loc[region_landslides['longitude'] < -115]\n",
        "\n",
        "# Clears the data from landslide triggers independant from the features used\n",
        "unusable_triggers = ['unknown', 'mining', 'monsoon', 'construction', 'earthquake', 'no_apparent_trigger', 'other', 'dam_embankment_collapse', 'leaking_pipe', 'volcano', 'vibration']\n",
        "\n",
        "for trigger in unusable_triggers:\n",
        "  trigger_unusable_idxs = region_landslides[region_landslides.landslide_trigger == trigger].index\n",
        "  region_landslides.drop(trigger_unusable_idxs, inplace=True)\n",
        "\n",
        "# Removes the data previous to 2005 for continuity with the other data\n",
        "threshold = 2005\n",
        "\n",
        "for idx, event in region_landslides.iterrows():\n",
        "  date = event.event_date.split()[0]\n",
        "  year = int(date.split('-')[0])\n",
        "  if year <= threshold:\n",
        "    print('Dropping event due to year, id:', idx, '- year:', year)\n",
        "    region_landslides.drop(idx, inplace=True)\n",
        "\n",
        "print('\\n\\n')\n",
        "\n",
        "region_landslides.value_counts(subset='landslide_trigger')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropping event due to year, id: 522 - year: 1997\n",
            "Dropping event due to year, id: 720 - year: 1997\n",
            "Dropping event due to year, id: 1059 - year: 1997\n",
            "Dropping event due to year, id: 1482 - year: 1998\n",
            "Dropping event due to year, id: 2028 - year: 1996\n",
            "Dropping event due to year, id: 2213 - year: 1998\n",
            "Dropping event due to year, id: 2534 - year: 1997\n",
            "Dropping event due to year, id: 2576 - year: 1997\n",
            "Dropping event due to year, id: 2579 - year: 1998\n",
            "Dropping event due to year, id: 3411 - year: 1997\n",
            "Dropping event due to year, id: 4575 - year: 1996\n",
            "Dropping event due to year, id: 4960 - year: 1998\n",
            "Dropping event due to year, id: 5000 - year: 1998\n",
            "Dropping event due to year, id: 5947 - year: 1997\n",
            "Dropping event due to year, id: 6197 - year: 1998\n",
            "Dropping event due to year, id: 6723 - year: 1998\n",
            "Dropping event due to year, id: 7883 - year: 1998\n",
            "Dropping event due to year, id: 8136 - year: 1998\n",
            "Dropping event due to year, id: 8164 - year: 1997\n",
            "Dropping event due to year, id: 8628 - year: 1998\n",
            "Dropping event due to year, id: 9232 - year: 1998\n",
            "Dropping event due to year, id: 9379 - year: 1998\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "landslide_trigger\n",
              "rain                 319\n",
              "downpour             302\n",
              "continuous_rain       37\n",
              "snowfall_snowmelt     26\n",
              "flooding              12\n",
              "freeze_thaw           10\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "j0fDzWnlr8hd",
        "outputId": "25904907-745d-43b2-d9da-c83892f91f25"
      },
      "source": [
        "# Creates a new dataframe with only the features useful for the model\n",
        "event_dates = region_landslides.event_date\n",
        "latitudes = region_landslides.latitude\n",
        "longitudes = region_landslides.longitude\n",
        "region_landslides_cleaned = pd.DataFrame({'dates': event_dates, 'lat': latitudes, 'long': longitudes})\n",
        "region_landslides_cleaned.head()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dates</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2009-01-02 02:00:00</td>\n",
              "      <td>45.4200</td>\n",
              "      <td>-122.6630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2012-03-30 00:00:00</td>\n",
              "      <td>48.2797</td>\n",
              "      <td>-117.2665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2009-01-01 22:24:00</td>\n",
              "      <td>45.3770</td>\n",
              "      <td>-122.0704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2009-01-03 00:00:00</td>\n",
              "      <td>45.5210</td>\n",
              "      <td>-122.6700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2009-01-07 00:00:00</td>\n",
              "      <td>47.4320</td>\n",
              "      <td>-122.3340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  dates      lat      long\n",
              "1   2009-01-02 02:00:00  45.4200 -122.6630\n",
              "6   2012-03-30 00:00:00  48.2797 -117.2665\n",
              "11  2009-01-01 22:24:00  45.3770 -122.0704\n",
              "13  2009-01-03 00:00:00  45.5210 -122.6700\n",
              "16  2009-01-07 00:00:00  47.4320 -122.3340"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrLaglcIKVH1"
      },
      "source": [
        "# Save the dataframe as csv\n",
        "'''\n",
        "region_landslides_cleaned.to_csv('landslides.csv')\n",
        "'''"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9VxfNJckOwh",
        "cellView": "code"
      },
      "source": [
        "# Data initialization for search of best region of interest\n",
        "'''\n",
        "min_lat = 20\n",
        "max_lat = 50\n",
        "min_long = -125\n",
        "max_long = -65\n",
        "\n",
        "count_region = np.zeros([(max_lat - min_lat)//5, (max_long - min_long)//5])\n",
        "print(count_region)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLEbsgOGjzJJ"
      },
      "source": [
        "# Algorithm to detect the distribution in the area of the US of landslides and to define temporal range\n",
        "'''\n",
        "earliest_year = 2021\n",
        "earliest_month = 13\n",
        "earliest_day = 32\n",
        "earliest_idx = 0\n",
        "\n",
        "for idx, event in region_landslides.iterrows():\n",
        "  if event.country_name == 'United States' and min_lat <= event.latitude <= max_lat and min_long <= event.longitude <= max_long:\n",
        "    x = int((event.latitude - min_lat)//5)\n",
        "    y = int((event.longitude - min_long)//5)\n",
        "    count_region[x][y] += 1\n",
        "    if 4 <= x <= 5 and 0 <= y <= 1:\n",
        "      try:\n",
        "        date = event.event_date.split()[0]\n",
        "      except AttributeError:\n",
        "        pass\n",
        "      year = int(date.split('-')[0])\n",
        "      if year <= earliest_year:\n",
        "        month = int(date.split('-')[1])\n",
        "        if year < earliest_year or (year == earliest_year and month <= earliest_month):\n",
        "          day = int(date.split('-')[2])\n",
        "          if month < earliest_month or (month == earliest_month and day < earliest_day):\n",
        "            earliest_day = day\n",
        "            earliest_idx = idx\n",
        "          earliest_month = month\n",
        "        earliest_year = year\n",
        "\n",
        "print(f'{earliest_year}-{earliest_month}-{earliest_day}')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Fbej1vv2o8"
      },
      "source": [
        "## Slope data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddA9fItmwAQE"
      },
      "source": [
        "slope_data = pd.read_csv('slope.csv')\n",
        "'''\n",
        "This dataset represents a matrix holding the maximum slope of a point in the\n",
        "region selected, as of now this represents ome of the biggest files, and\n",
        "therefore one of the hardest to handle, but it is luckily the one that requires\n",
        "less maintenance due to the small variations of these values in short periods\n",
        "of time\n",
        "\n",
        "source: https://www.eorc.jaxa.jp/ALOS/en/aw3d30/index.htm (this is only an\n",
        "elevation map, the data is then processed to obtain the gradient and derive the\n",
        "steepest slope)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq9SKETewgyD"
      },
      "source": [
        "## Vegetation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPv5QqQ9woJM"
      },
      "source": [
        "vegetation_data = pd.read_csv('vegetation.csv')\n",
        "'''\n",
        "This dataset comes from the Nasa Earth Observations, which uses the MODIS\n",
        "satellite to determine an index describing the vegetation distribution on a\n",
        "global scale.\n",
        "This file requires updating every 16 days/1 month due to the high activity of\n",
        "wildfires and human deforestation.\n",
        "\n",
        "source: https://neo.sci.gsfc.nasa.gov/archive/csv/MOD_NDVI_16/\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpuccmTaw0Uz"
      },
      "source": [
        "## Rain data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnsXnKEWw15v"
      },
      "source": [
        "rain_data = pd.read_csv('rain.csv')\n",
        "'''\n",
        "This dataset is the biggest one, but differently than the data regarding the\n",
        "terrain slope it can downloaded through an API, requiring therefore only some\n",
        "data preprocessing.\n",
        "This dataset hosts the hourly precipitations of a specific point in the span of\n",
        "a month, and it can be used for both the istantaneous rainfall and the\n",
        "cumulative rainfalls, two important factors in landslides.\n",
        "\n",
        "source: https://sharaku.eorc.jaxa.jp/GSMaP/index.htm\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duW0l32_w68E"
      },
      "source": [
        "## Terrain temperature data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epGuAxw_w-F7"
      },
      "source": [
        "terrain_temperature_data = pd.read_csv('terrain_temperature.csv')\n",
        "'''\n",
        "Terrain temperature dataset also obtained from the MODIS satellite through the\n",
        "Nasa Earth Observations, it can also be obtained through API, making it not as\n",
        "problematic as the terrain slope.\n",
        "This dataset hosts the temperature of the terrain, giving us a good insight on\n",
        "type of the terrain and how it is affected by rainfalls, an important parameter\n",
        "for landslides caused by precipitations.\n",
        "\n",
        "source: https://neo.sci.gsfc.nasa.gov/archive/csv/MOD_LSTD_D/\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CvBZR7MxITE"
      },
      "source": [
        "# Dataset creation and preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBar75rLzT7T"
      },
      "source": [
        "## Combine data into positive dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XfEfL9IzYkq"
      },
      "source": [
        "''' INSTRUCTIONS:\n",
        "create dataset with data {rain_daily, rain_total, vegetation, temperature, slope, landslide(output)}\n",
        "\n",
        "for each element of the landslide dataset:\n",
        "  get latitude, longitude and date\n",
        "  get inputs from the data already read:\n",
        "    for vegetation, check if the date is between two sets\n",
        "    for rain, same, but the time can be probably hourly or interpolated\n",
        "    for temperature just check the day\n",
        "    for slope, use the latitude and longitude only\n",
        "  add the inputs to the dataset, and consider landslide = 1\n",
        "'''\n",
        "\n",
        "'''\n",
        "Here the dataset of positive values is composed by using the data obtained from\n",
        "the landslides repository after cleaning, and obtaining the specific features at\n",
        "the spot of the event.\n",
        "We will consider this as the positive output (1) of our binary classifier.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCLFv6vn4wva"
      },
      "source": [
        "## Create negative dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prGfPfid42zR"
      },
      "source": [
        "''' INSTRUCTIONS:\n",
        "for 5000 times:\n",
        "  random temporal value between 2006 and 2020\n",
        "  random latitude between 40 and 50\n",
        "  random longitude between -125 and -115\n",
        "\n",
        "  new empty dataset with the same indexes as before\n",
        "\n",
        "  check it the altitude is >0, maybe in the slope we could use -1 to make it\n",
        "  readable directly from it\n",
        "\n",
        "  check if the location had a landslide (passing through all the data for the \n",
        "  landslides might be a little slow, but I haven't come up with another idea yet)\n",
        "\n",
        "  if not, get the inputs from the data\n",
        "  append the inputs to the dataset, with landslide=0\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "For the negative class we have to create a dataset using the data we collected,\n",
        "therefore we decided to create a random set of coordinates (both spatial and\n",
        "temporal) and use those to define a new set of inputs for a new entry.\n",
        "Before actually considering this entry although we check if this entry coincides\n",
        "with one of the landslide events, or if the location decided is in a body of\n",
        "water (defined through the slope through a particular tag, for example -1)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz2peilaGbGc"
      },
      "source": [
        "# Combine the two datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVXHY5Z8GvTN"
      },
      "source": [
        "''' INSTRUCTIONS:\n",
        "from the positive dataset take out 50 random elements\n",
        "from the negative dataset take out 50 random elements\n",
        "\n",
        "combine these into the test dataset\n",
        "\n",
        "divide again the two datasets into validation and training (5% and 95%) and\n",
        "combine the positive and negative parts\n",
        "\n",
        "copy all the elements that have landslide=1 and append them twice to the dataset\n",
        "(oversampling, the training will contain 3 copies for each landslide event)\n",
        "(This with both validation and training sets)\n",
        "\n",
        "shuffle validation and training set\n",
        "'''\n",
        "\n",
        "'''\n",
        "As we have a really skewed dataset, the process of combining the positive (from\n",
        "data) and negative (randomly generated) datasets is more complex.\n",
        "We initially removed from both sets a few entries to use as a test dataset and\n",
        "set those apart until scaling.\n",
        "The remaining data is then divided into validation and training (the validation\n",
        "is relatively small compared to the training, as to not reduce further the\n",
        "amount of positive examples) and in each of the sets the positive part is\n",
        "oversampled, meaning that 2 more copies of it are added to the dataset,\n",
        "increasing the ratio between positive and negative examples.\n",
        "The separation before the oversampling assure us that there won't be leakage of\n",
        "data between the different sets, as this could preclude the performance testing\n",
        "later.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyzlYmvWMXCt"
      },
      "source": [
        "## Scaling the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeF1HiujMaE6"
      },
      "source": [
        "''' INSTRUCTIONS:\n",
        "for each column:\n",
        "  max(max(training) - min(training), max(validation) - min(validation), max(test) - min(test))\n",
        "  min(min(training), min(validation), min(test))\n",
        "  subtract the minimum value and divide all the elements of that column for the max\n",
        "\n",
        "  OR\n",
        "\n",
        "  standardization (sklearn.preprocessing.StandardScaler)\n",
        "'''\n",
        "\n",
        "'''\n",
        "To maintain the same dimensions throughout the datasets, we decided to\n",
        "standardize all the values (removing the mean value and rescaling by the\n",
        "standard deviation) based on the maximum of the three sets.\n",
        "This assures us that the model will work fluently even on the other sets.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic-YZTTQJgzo"
      },
      "source": [
        "# ML model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUX7DYs1UWiD"
      },
      "source": [
        "# Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTClb6jbSJmS"
      },
      "source": [
        "''' INSTRUCTIONS:\n",
        "define structure using the sequential model\n",
        "a few dense layers, 'relu' as a function should be good.\n",
        "'''\n",
        "model = Sequential([\n",
        "                    Dense(units=16, activation='relu', input_shape=(5, ),\n",
        "                    Dense(units=32, activation='relu'),\n",
        "                    Dense(units=16, activation='relu'),\n",
        "                    Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "                    \n",
        "'''\n",
        "This is an initial idea for the architecture of the model, which isn't highly\n",
        "complex, but could suffice given the amount of features used as inputs.\n",
        "To define a better structure we would have to analyse the recall score, as we\n",
        "are interested to obtain the best results at guessing when the landslides are\n",
        "probable, as to inform authorities that could analyse the location through\n",
        "a more in-depth analysis.\n",
        "Given the type of inputs, a relu activation function seems like a good choice,\n",
        "as this can work similarly to a threshold, which seems to describe well the\n",
        "relation between our features and the event.\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i7U4fQKUA_g"
      },
      "source": [
        "''' INSTRUCTIONS:\n",
        "show the results on a precision/recall curve\n",
        "\n",
        "test the model on the test data\n",
        "'''\n",
        "\n",
        "'''\n",
        "In this section we would mostly just look at the results (recall, F1 score etc.)\n",
        "as to determine the worth of the model.\n",
        "Analysing the model on data that has never been seen (test data) would bring us\n",
        "close to a real-world test, showing us the performances of our dataset on a \n",
        "problem at a regional level\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlnp4zXxVcPX"
      },
      "source": [
        "# Further Developements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3gBMqP7Vhfx"
      },
      "source": [
        "As we are approaching the end of the challenge it is clearly visible that the amount of time required to clean and process the data, to build the infrastructure to obtain data through API and to train and test the model is far more than the one available to us, even more so considering our inexperience with data science."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkSkJslIekD0"
      },
      "source": [
        "## Additional work on the model\n",
        "We decided to focus our energies more on verifying that the actual project would be feasible more than actually programming, therefore a lot of improvements could be made first of all on the model itself, but even more on the features decided, as more complex features, such as soil moisture and type of terrain could grant us a much greater insight on the reasons behind water-born landslides."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIWCfhngeYuT"
      },
      "source": [
        "## Availability to the public\n",
        "As of now we decided to present the results of the prediction through an online map, as internet is getting more readily available to the world population day by day, allowing us to reach rural communities that wouldn't be able to get such vital informations otherwise.\n",
        "\n",
        "With time this project could land as a mobile application or possibly as an analytical system for governement agencies, allowing for faster intervention and prevention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7A-jUEVeawj"
      },
      "source": [
        "## Data gathering\n",
        "Currently the amount of landslides categorized is not immense, so a further application for this project could see the ability for users to declare a landslide happening in the nearby area, granting faster information sharing while also improving the data available to the model."
      ]
    }
  ]
}