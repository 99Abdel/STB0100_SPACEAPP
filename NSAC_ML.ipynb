{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NSAC - ML.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vOszr2E9wJ_0",
        "X5TTHGXbwa-c",
        "t6Fbej1vv2o8",
        "Dq9SKETewgyD",
        "qpuccmTaw0Uz",
        "duW0l32_w68E"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMXIh6uYdIE7bSblc/KKe2Y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKZTkylySCcr"
      },
      "source": [
        "# Guide"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grkvUOWMJfVZ"
      },
      "source": [
        "## IMPORTS\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "## LOADING\n",
        "# Load vegetation data\n",
        "# Load rain data\n",
        "# Load altitude data\n",
        "# Load temperature data\n",
        "# X Load landslides data\n",
        "\n",
        "## DATA PREPROCESSING\n",
        "# create slope data from altitude\n",
        "# scale data in a 0 to 1 range\n",
        "\n",
        "# X clear landslides data from useless data\n",
        "# X filter values outside the specified region (50N - 40N, 115W - 125W)\n",
        "# X filter non-water related landslides\n",
        "# obtain data from other datasets to complete the positive values of the set\n",
        "\n",
        "## NEGATIVE DATA CREATION\n",
        "# Generate random location\n",
        "# Check if the location is available (h>0)\n",
        "# Generate random date/hour\n",
        "# Check if there wasn't a landslide\n",
        "\n",
        "## CREATE TRAINING, VALIDATION AND TEST SETS\n",
        "# stratified separation for validation and test sets\n",
        "# oversampling landslides events (x3?) to account for imbalance\n",
        "# shuffle training and validation\n",
        "\n",
        "## DEFINE MODEL\n",
        "# weighted loss function\n",
        "# dense deep neural network (architecture?)\n",
        "# binary classification (which function? sigmoid?)\n",
        "\n",
        "## TEST PREDICTIONS\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOszr2E9wJ_0"
      },
      "source": [
        "# Preparation of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5TTHGXbwa-c"
      },
      "source": [
        "## Landslides data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anKlBpUSaYSd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "landslide_data = pd.read_csv('/content/nasa_global_landslide_catalog_point.csv')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebg0smiBatgk",
        "outputId": "9ce28537-63d4-4d7a-ac2e-1b6e37960e74"
      },
      "source": [
        "# Overlook at the dataset\n",
        "print(landslide_data.axes[1])\n",
        "# List of triggers\n",
        "landslide_data['landslide_trigger'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['OBJECTID', 'Shape', 'source_name', 'source_link', 'event_id',\n",
            "       'event_date', 'event_time', 'event_title', 'event_description',\n",
            "       'location_description', 'location_accuracy', 'landslide_category',\n",
            "       'landslide_trigger', 'landslide_size', 'landslide_setting',\n",
            "       'fatality_count', 'injury_count', 'storm_name', 'photo_link',\n",
            "       'comments', 'event_import_source', 'event_import_id', 'latitude',\n",
            "       'longitude', 'country_name', 'country_code', 'admin_division_name',\n",
            "       'gazetteer_closest_point', 'gazetteer_distance', 'submitted_date',\n",
            "       'last_edited_date'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "downpour                   4794\n",
              "unknown                    2842\n",
              "rain                       2716\n",
              "continuous_rain             780\n",
              "tropical_cyclone            563\n",
              "snowfall_snowmelt           147\n",
              "monsoon                     133\n",
              "mining                      103\n",
              "construction                 95\n",
              "earthquake                   95\n",
              "flooding                     81\n",
              "no_apparent_trigger          65\n",
              "freeze_thaw                  44\n",
              "other                        36\n",
              "dam_embankment_collapse      15\n",
              "leaking_pipe                 13\n",
              "volcano                       2\n",
              "vibration                     2\n",
              "Name: landslide_trigger, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoxOZIb9jFmA",
        "outputId": "eb931165-7a94-4e98-c951-678b78e43d3c"
      },
      "source": [
        "# Restricts the area to 50N125W - 40N115W\n",
        "us_landslides = landslide_data.loc[landslide_data['country_name'] == 'United States'].dropna(subset=['event_date'])\n",
        "region_landslides = us_landslides.loc[us_landslides['latitude'] > 40]\n",
        "region_landslides = region_landslides.loc[region_landslides['latitude'] < 50]\n",
        "region_landslides = region_landslides.loc[region_landslides['longitude'] > -125]\n",
        "region_landslides = region_landslides.loc[region_landslides['longitude'] < -115]\n",
        "\n",
        "# Clears the data from landslide triggers independant from the features used\n",
        "unusable_triggers = ['unknown', 'mining', 'monsoon', 'construction', 'earthquake', 'no_apparent_trigger', 'other', 'dam_embankment_collapse', 'leaking_pipe', 'volcano', 'vibration']\n",
        "\n",
        "for trigger in unusable_triggers:\n",
        "  trigger_unusable_idxs = region_landslides[region_landslides.landslide_trigger == trigger].index\n",
        "  region_landslides.drop(trigger_unusable_idxs, inplace=True)\n",
        "\n",
        "# Removes the data previous to 2005 for continuity with the other data\n",
        "threshold = 2005\n",
        "\n",
        "for idx, event in region_landslides.iterrows():\n",
        "  date = event.event_date.split()[0]\n",
        "  year = int(date.split('-')[0])\n",
        "  if year <= threshold:\n",
        "    print('Dropping event due to year, id:', idx, '- year:', year)\n",
        "    region_landslides.drop(idx, inplace=True)\n",
        "\n",
        "print('\\n\\n')\n",
        "\n",
        "region_landslides.value_counts(subset='landslide_trigger')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropping event due to year, id: 522 - year: 1997\n",
            "Dropping event due to year, id: 720 - year: 1997\n",
            "Dropping event due to year, id: 1059 - year: 1997\n",
            "Dropping event due to year, id: 1482 - year: 1998\n",
            "Dropping event due to year, id: 2028 - year: 1996\n",
            "Dropping event due to year, id: 2213 - year: 1998\n",
            "Dropping event due to year, id: 2534 - year: 1997\n",
            "Dropping event due to year, id: 2576 - year: 1997\n",
            "Dropping event due to year, id: 2579 - year: 1998\n",
            "Dropping event due to year, id: 3411 - year: 1997\n",
            "Dropping event due to year, id: 4575 - year: 1996\n",
            "Dropping event due to year, id: 4960 - year: 1998\n",
            "Dropping event due to year, id: 5000 - year: 1998\n",
            "Dropping event due to year, id: 5947 - year: 1997\n",
            "Dropping event due to year, id: 6197 - year: 1998\n",
            "Dropping event due to year, id: 6723 - year: 1998\n",
            "Dropping event due to year, id: 7883 - year: 1998\n",
            "Dropping event due to year, id: 8136 - year: 1998\n",
            "Dropping event due to year, id: 8164 - year: 1997\n",
            "Dropping event due to year, id: 8628 - year: 1998\n",
            "Dropping event due to year, id: 9232 - year: 1998\n",
            "Dropping event due to year, id: 9379 - year: 1998\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "landslide_trigger\n",
              "rain                 319\n",
              "downpour             302\n",
              "continuous_rain       37\n",
              "snowfall_snowmelt     26\n",
              "flooding              12\n",
              "freeze_thaw           10\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "j0fDzWnlr8hd",
        "outputId": "25904907-745d-43b2-d9da-c83892f91f25"
      },
      "source": [
        "# Creates a new dataframe with only the features useful for the model\n",
        "event_dates = region_landslides.event_date\n",
        "latitudes = region_landslides.latitude\n",
        "longitudes = region_landslides.longitude\n",
        "region_landslides_cleaned = pd.DataFrame({'dates': event_dates, 'lat': latitudes, 'long': longitudes})\n",
        "region_landslides_cleaned.head()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dates</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2009-01-02 02:00:00</td>\n",
              "      <td>45.4200</td>\n",
              "      <td>-122.6630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2012-03-30 00:00:00</td>\n",
              "      <td>48.2797</td>\n",
              "      <td>-117.2665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2009-01-01 22:24:00</td>\n",
              "      <td>45.3770</td>\n",
              "      <td>-122.0704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2009-01-03 00:00:00</td>\n",
              "      <td>45.5210</td>\n",
              "      <td>-122.6700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2009-01-07 00:00:00</td>\n",
              "      <td>47.4320</td>\n",
              "      <td>-122.3340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  dates      lat      long\n",
              "1   2009-01-02 02:00:00  45.4200 -122.6630\n",
              "6   2012-03-30 00:00:00  48.2797 -117.2665\n",
              "11  2009-01-01 22:24:00  45.3770 -122.0704\n",
              "13  2009-01-03 00:00:00  45.5210 -122.6700\n",
              "16  2009-01-07 00:00:00  47.4320 -122.3340"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrLaglcIKVH1"
      },
      "source": [
        "# Save the dataframe as csv\n",
        "'''\n",
        "region_landslides_cleaned.to_csv('landslides.csv')\n",
        "'''"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9VxfNJckOwh",
        "cellView": "code"
      },
      "source": [
        "# Data initialization for search of best region of interest\n",
        "'''\n",
        "min_lat = 20\n",
        "max_lat = 50\n",
        "min_long = -125\n",
        "max_long = -65\n",
        "\n",
        "count_region = np.zeros([(max_lat - min_lat)//5, (max_long - min_long)//5])\n",
        "print(count_region)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLEbsgOGjzJJ"
      },
      "source": [
        "# Algorithm to detect the distribution in the area of the US of landslides and to define temporal range\n",
        "'''\n",
        "earliest_year = 2021\n",
        "earliest_month = 13\n",
        "earliest_day = 32\n",
        "earliest_idx = 0\n",
        "\n",
        "for idx, event in region_landslides.iterrows():\n",
        "  if event.country_name == 'United States' and min_lat <= event.latitude <= max_lat and min_long <= event.longitude <= max_long:\n",
        "    x = int((event.latitude - min_lat)//5)\n",
        "    y = int((event.longitude - min_long)//5)\n",
        "    count_region[x][y] += 1\n",
        "    if 4 <= x <= 5 and 0 <= y <= 1:\n",
        "      try:\n",
        "        date = event.event_date.split()[0]\n",
        "      except AttributeError:\n",
        "        pass\n",
        "      year = int(date.split('-')[0])\n",
        "      if year <= earliest_year:\n",
        "        month = int(date.split('-')[1])\n",
        "        if year < earliest_year or (year == earliest_year and month <= earliest_month):\n",
        "          day = int(date.split('-')[2])\n",
        "          if month < earliest_month or (month == earliest_month and day < earliest_day):\n",
        "            earliest_day = day\n",
        "            earliest_idx = idx\n",
        "          earliest_month = month\n",
        "        earliest_year = year\n",
        "\n",
        "print(f'{earliest_year}-{earliest_month}-{earliest_day}')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "082eO_6lwPRU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Fbej1vv2o8"
      },
      "source": [
        "## Slope data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddA9fItmwAQE"
      },
      "source": [
        "slope_data = pd.read_csv('slope.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq9SKETewgyD"
      },
      "source": [
        "## Vegetation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPv5QqQ9woJM"
      },
      "source": [
        "vegetation_data = pd.read_csv('vegetation.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpuccmTaw0Uz"
      },
      "source": [
        "## Rain data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnsXnKEWw15v"
      },
      "source": [
        "rain_data = pd.read_csv('rain.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duW0l32_w68E"
      },
      "source": [
        "## Terrain temperature data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epGuAxw_w-F7"
      },
      "source": [
        "terrain_temperature_data = pd.read_csv('terrain_temperature.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CvBZR7MxITE"
      },
      "source": [
        "# Dataset creation and preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBar75rLzT7T"
      },
      "source": [
        "## Combine data into positive dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XfEfL9IzYkq"
      },
      "source": [
        "''' INSTRUCTIONS:\n",
        "create dataset with data {rain_daily, rain_total, vegetation, temperature, slope, landslide(output)}\n",
        "\n",
        "for each element of the landslide dataset:\n",
        "  get latitude, longitude and date\n",
        "  get inputs from the data already read:\n",
        "    for vegetation, check if the date is between two sets\n",
        "    for rain, same, but the time can be probably hourly or interpolated\n",
        "    for temperature just check the day\n",
        "    for slope, use the latitude and longitude only\n",
        "  add the inputs to the dataset, and consider landslide = 1\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCLFv6vn4wva"
      },
      "source": [
        "## Create negative dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prGfPfid42zR"
      },
      "source": [
        "''' INSTRUCTIONS:\n",
        "for 5000 times:\n",
        "  random temporal value between 2006 and 2020\n",
        "  random latitude between 40 and 50\n",
        "  random longitude between -125 and -115\n",
        "\n",
        "  new empty dataset with the same indexes as before\n",
        "\n",
        "  check it the altitude is >0, maybe in the slope we could use -1 to make it\n",
        "  readable directly from it\n",
        "\n",
        "  check if the location had a landslide (passing through all the data for the \n",
        "  landslides might be a little slow, but I haven't come up with another idea yet)\n",
        "\n",
        "  if not, get the inputs from the data\n",
        "  append the inputs to the dataset, with landslide=0\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz2peilaGbGc"
      },
      "source": [
        "# Combine the two datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVXHY5Z8GvTN"
      },
      "source": [
        "''' INSTRUCTIONS:\n",
        "from the positive dataset take out 50 random elements\n",
        "from the negative dataset take out 50 random elements\n",
        "\n",
        "combine these into the test dataset\n",
        "\n",
        "divide again the two datasets into validation and training (5% and 95%) and\n",
        "combine the positive and negative parts\n",
        "\n",
        "copy all the elements that have landslide=1 and append them twice to the dataset\n",
        "(oversampling, the training will contain 3 copies for each landslide event)\n",
        "(This with both validation and training sets)\n",
        "\n",
        "shuffle validation and training set\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyzlYmvWMXCt"
      },
      "source": [
        "## Scaling the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeF1HiujMaE6"
      },
      "source": [
        "''' INSTRUCTIONS:\n",
        "for each column:\n",
        "  max(max(training) - min(training), max(validation) - min(validation), max(test) - min(test))\n",
        "  min(min(training), min(validation), min(test))\n",
        "  subtract the minimum value and divide all the elements of that column for the max\n",
        "\n",
        "  OR\n",
        "\n",
        "  standardization (sklearn.preprocessing.StandardScaler)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic-YZTTQJgzo"
      },
      "source": [
        "# ML model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUX7DYs1UWiD"
      },
      "source": [
        "# Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTClb6jbSJmS"
      },
      "source": [
        "''' INSTRUCTIONS:\n",
        "define structure using the sequential model\n",
        "a few dense layers, 'relu' as a function should be good\n",
        "(size increasing with depth, so for example 20, 40, 60)\n",
        "'''\n",
        "model = Sequential([\n",
        "                    Dense(units=16, activation='relu', input_shape=(5, ),\n",
        "                    Dense(units=32, activation='relu'),\n",
        "                    Dense(units=16, activation='relu'),\n",
        "                    Dense(units=1, activation='')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i7U4fQKUA_g"
      },
      "source": [
        "''' INSTRUCTIONS:\n",
        "show the results on a precision/recall curve\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}